{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import *\n",
    "\n",
    "# model_class, tokenizer_class, pretrained_weights  = (BertModel, BertTokenizer, 'bert-base-uncased')\n",
    "# tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "# model = model_class.from_pretrained(pretrained_weights).cuda()\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "I0217 12:36:29.267818 140543562405696 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I0217 12:36:30.364857 140543562405696 tokenization_utils.py:375] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ds_user1/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0217 12:36:31.187318 140543562405696 configuration_utils.py:152] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ds_user1/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0217 12:36:31.189495 140543562405696 configuration_utils.py:169] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0217 12:36:31.960604 140543562405696 modeling_utils.py:387] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ds_user1/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "from bert_model import *\n",
    "my_model = mymodel().cuda()\n",
    "my_model.train()\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "lr = 1e-3\n",
    "max_grad_norm = 1.0\n",
    "num_training_steps = 1000\n",
    "num_warmup_steps = 100\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(my_model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_label = open('main_action.txt')\n",
    "labels = f_label.readlines()\n",
    "gts = [x.strip() for x in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('corpus_100000.txt')\n",
    "texts = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(texts)):\n",
    "for i in range(1):\n",
    "    label = texts[i].split('\\t')[0]    \n",
    "    sen = texts[i].split('\\t')[1].strip()\n",
    "    \n",
    "    label_idx = gts.index(label)\n",
    "    torch_label = torch.from_numpy(np.asarray([label_idx])).cuda() \n",
    "    \n",
    "    bert_out = my_model.bert_layer(sen) # (batch, seq_len, emb_dim)    \n",
    "#     model_out = model(sen_idx)[0] \n",
    "    fc_out = my_model.fc_layer(bert_out)\n",
    "    cls_out = fc_out[:,0:1,:].squeeze(1)\n",
    "    \n",
    "    loss = my_model.cls_loss(torch_label, cls_out)\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(my_model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embeddings.word_embeddings.weight tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
      "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
      "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
      "        ...,\n",
      "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
      "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
      "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
      "       device='cuda:0')\n",
      "model.encoder.layer.1.attention.self.query.weight tensor([[ 0.0303, -0.0246,  0.0064,  ..., -0.0333, -0.0601,  0.0664],\n",
      "        [-0.0123, -0.0221, -0.0302,  ..., -0.0324,  0.0519,  0.0496],\n",
      "        [-0.0740, -0.0510, -0.0088,  ...,  0.0180,  0.0118, -0.0496],\n",
      "        ...,\n",
      "        [ 0.0563, -0.0185, -0.0335,  ...,  0.0111,  0.0094, -0.0260],\n",
      "        [ 0.0086, -0.0139, -0.0069,  ..., -0.0046,  0.0224, -0.0608],\n",
      "        [-0.0080, -0.0141,  0.0540,  ...,  0.1033, -0.0529,  0.0210]],\n",
      "       device='cuda:0')\n",
      "matrix.weight tensor([[-0.0304,  0.0378, -0.0328,  ..., -0.0244, -0.0018, -0.0086],\n",
      "        [-0.0083,  0.0065, -0.0017,  ...,  0.0275, -0.0308, -0.0097],\n",
      "        [ 0.0169,  0.0334,  0.0142,  ...,  0.0357,  0.0145, -0.0238],\n",
      "        ...,\n",
      "        [-0.0195,  0.0127,  0.0192,  ...,  0.0118,  0.0206, -0.0147],\n",
      "        [ 0.0263, -0.0147, -0.0140,  ..., -0.0288, -0.0377,  0.0122],\n",
      "        [ 0.0181, -0.0194, -0.0038,  ..., -0.0216, -0.0311, -0.0205]],\n",
      "       device='cuda:0')\n",
      "matrix.bias tensor([-0.0326, -0.0100,  0.0030, -0.0351,  0.0055,  0.0096, -0.0173, -0.0143,\n",
      "        -0.0080, -0.0228, -0.0244,  0.0072,  0.0049,  0.0227, -0.0402, -0.0299,\n",
      "         0.0219, -0.0237, -0.0236, -0.0025,  0.0113,  0.0033, -0.0249,  0.0205,\n",
      "         0.0236, -0.0051,  0.0007, -0.0072,  0.0094, -0.0261, -0.0268, -0.0209,\n",
      "        -0.0266, -0.0295,  0.0269, -0.0419, -0.0371,  0.0220, -0.0241,  0.0111,\n",
      "         0.0041, -0.0144, -0.0418, -0.0031,  0.0123,  0.0237, -0.0182,  0.0197,\n",
      "         0.0199, -0.0416,  0.0249,  0.0154,  0.0263,  0.0109,  0.0212, -0.0092,\n",
      "        -0.0005,  0.0088,  0.0028, -0.0242,  0.0081, -0.0422,  0.0024, -0.0439,\n",
      "         0.0210,  0.0002,  0.0173, -0.0246,  0.0238,  0.0019,  0.0195,  0.0050,\n",
      "        -0.0143, -0.0017,  0.0189, -0.0169,  0.0012, -0.0232, -0.0166,  0.0257,\n",
      "        -0.0079,  0.0089, -0.0390, -0.0417, -0.0345, -0.0066, -0.0159, -0.0401,\n",
      "         0.0174, -0.0045, -0.0030, -0.0345, -0.0286, -0.0326, -0.0282, -0.0135,\n",
      "         0.0044, -0.0157, -0.0056, -0.0021, -0.0376, -0.0246,  0.0211, -0.0218,\n",
      "         0.0022, -0.0383,  0.0133,  0.0007,  0.0250, -0.0298, -0.0116, -0.0078,\n",
      "        -0.0121,  0.0180,  0.0139,  0.0133,  0.0198,  0.0215, -0.0016, -0.0106,\n",
      "         0.0235, -0.0350,  0.0155, -0.0364, -0.0026, -0.0282, -0.0445,  0.0196,\n",
      "         0.0178,  0.0254, -0.0030, -0.0082,  0.0147,  0.0251,  0.0018,  0.0037,\n",
      "         0.0102,  0.0194,  0.0166,  0.0018,  0.0252, -0.0120,  0.0181, -0.0279,\n",
      "        -0.0446, -0.0176,  0.0216, -0.0335,  0.0199,  0.0038, -0.0130,  0.0225,\n",
      "         0.0093, -0.0273,  0.0064, -0.0274,  0.0062,  0.0203, -0.0050, -0.0270,\n",
      "        -0.0281,  0.0029, -0.0125, -0.0326, -0.0295, -0.0112, -0.0309, -0.0455,\n",
      "        -0.0318,  0.0246,  0.0235, -0.0156, -0.0178,  0.0001, -0.0211, -0.0259,\n",
      "         0.0232, -0.0340,  0.0214,  0.0233,  0.0057, -0.0177, -0.0089, -0.0159,\n",
      "        -0.0175, -0.0374, -0.0387, -0.0027,  0.0079, -0.0314, -0.0144, -0.0137,\n",
      "        -0.0363], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in my_model.named_parameters():\n",
    "    if name == 'model.embeddings.word_embeddings.weight':\n",
    "        print(name, param.data)\n",
    "    if name == 'model.encoder.layer.1.attention.self.query.weight':\n",
    "        print(name, param.data)\n",
    "    if name == 'matrix.weight':\n",
    "        print(name, param.data)\n",
    "    if name == 'matrix.bias':\n",
    "        print(name, param.data)        \n",
    "        \n",
    "\n",
    "        \n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0157,  0.0245, -0.0258,  ...,  0.0133,  0.0772,  0.0566],\n",
       "        [-0.0315,  0.0362, -0.0417,  ..., -0.0518,  0.1390,  0.0078],\n",
       "        [ 0.0118,  0.0329,  0.0117,  ..., -0.0295,  0.0256, -0.0448],\n",
       "        ...,\n",
       "        [-0.0087,  0.0491,  0.0566,  ...,  0.0256,  0.0526, -0.0541],\n",
       "        [-0.0213,  0.0922,  0.0627,  ..., -0.1049,  0.0589,  0.0448],\n",
       "        [ 0.0026, -0.0948,  0.0117,  ..., -0.0199, -0.0503, -0.0096]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(my_model.parameters())[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embeddings.word_embeddings.weight\n",
      "model.embeddings.position_embeddings.weight\n",
      "model.embeddings.token_type_embeddings.weight\n",
      "model.embeddings.LayerNorm.weight\n",
      "model.embeddings.LayerNorm.bias\n",
      "model.encoder.layer.0.attention.self.query.weight\n",
      "model.encoder.layer.0.attention.self.query.bias\n",
      "model.encoder.layer.0.attention.self.key.weight\n",
      "model.encoder.layer.0.attention.self.key.bias\n",
      "model.encoder.layer.0.attention.self.value.weight\n",
      "model.encoder.layer.0.attention.self.value.bias\n",
      "model.encoder.layer.0.attention.output.dense.weight\n",
      "model.encoder.layer.0.attention.output.dense.bias\n",
      "model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.0.intermediate.dense.weight\n",
      "model.encoder.layer.0.intermediate.dense.bias\n",
      "model.encoder.layer.0.output.dense.weight\n",
      "model.encoder.layer.0.output.dense.bias\n",
      "model.encoder.layer.0.output.LayerNorm.weight\n",
      "model.encoder.layer.0.output.LayerNorm.bias\n",
      "model.encoder.layer.1.attention.self.query.weight\n",
      "model.encoder.layer.1.attention.self.query.bias\n",
      "model.encoder.layer.1.attention.self.key.weight\n",
      "model.encoder.layer.1.attention.self.key.bias\n",
      "model.encoder.layer.1.attention.self.value.weight\n",
      "model.encoder.layer.1.attention.self.value.bias\n",
      "model.encoder.layer.1.attention.output.dense.weight\n",
      "model.encoder.layer.1.attention.output.dense.bias\n",
      "model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.1.intermediate.dense.weight\n",
      "model.encoder.layer.1.intermediate.dense.bias\n",
      "model.encoder.layer.1.output.dense.weight\n",
      "model.encoder.layer.1.output.dense.bias\n",
      "model.encoder.layer.1.output.LayerNorm.weight\n",
      "model.encoder.layer.1.output.LayerNorm.bias\n",
      "model.encoder.layer.2.attention.self.query.weight\n",
      "model.encoder.layer.2.attention.self.query.bias\n",
      "model.encoder.layer.2.attention.self.key.weight\n",
      "model.encoder.layer.2.attention.self.key.bias\n",
      "model.encoder.layer.2.attention.self.value.weight\n",
      "model.encoder.layer.2.attention.self.value.bias\n",
      "model.encoder.layer.2.attention.output.dense.weight\n",
      "model.encoder.layer.2.attention.output.dense.bias\n",
      "model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.2.intermediate.dense.weight\n",
      "model.encoder.layer.2.intermediate.dense.bias\n",
      "model.encoder.layer.2.output.dense.weight\n",
      "model.encoder.layer.2.output.dense.bias\n",
      "model.encoder.layer.2.output.LayerNorm.weight\n",
      "model.encoder.layer.2.output.LayerNorm.bias\n",
      "model.encoder.layer.3.attention.self.query.weight\n",
      "model.encoder.layer.3.attention.self.query.bias\n",
      "model.encoder.layer.3.attention.self.key.weight\n",
      "model.encoder.layer.3.attention.self.key.bias\n",
      "model.encoder.layer.3.attention.self.value.weight\n",
      "model.encoder.layer.3.attention.self.value.bias\n",
      "model.encoder.layer.3.attention.output.dense.weight\n",
      "model.encoder.layer.3.attention.output.dense.bias\n",
      "model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.3.intermediate.dense.weight\n",
      "model.encoder.layer.3.intermediate.dense.bias\n",
      "model.encoder.layer.3.output.dense.weight\n",
      "model.encoder.layer.3.output.dense.bias\n",
      "model.encoder.layer.3.output.LayerNorm.weight\n",
      "model.encoder.layer.3.output.LayerNorm.bias\n",
      "model.encoder.layer.4.attention.self.query.weight\n",
      "model.encoder.layer.4.attention.self.query.bias\n",
      "model.encoder.layer.4.attention.self.key.weight\n",
      "model.encoder.layer.4.attention.self.key.bias\n",
      "model.encoder.layer.4.attention.self.value.weight\n",
      "model.encoder.layer.4.attention.self.value.bias\n",
      "model.encoder.layer.4.attention.output.dense.weight\n",
      "model.encoder.layer.4.attention.output.dense.bias\n",
      "model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.4.intermediate.dense.weight\n",
      "model.encoder.layer.4.intermediate.dense.bias\n",
      "model.encoder.layer.4.output.dense.weight\n",
      "model.encoder.layer.4.output.dense.bias\n",
      "model.encoder.layer.4.output.LayerNorm.weight\n",
      "model.encoder.layer.4.output.LayerNorm.bias\n",
      "model.encoder.layer.5.attention.self.query.weight\n",
      "model.encoder.layer.5.attention.self.query.bias\n",
      "model.encoder.layer.5.attention.self.key.weight\n",
      "model.encoder.layer.5.attention.self.key.bias\n",
      "model.encoder.layer.5.attention.self.value.weight\n",
      "model.encoder.layer.5.attention.self.value.bias\n",
      "model.encoder.layer.5.attention.output.dense.weight\n",
      "model.encoder.layer.5.attention.output.dense.bias\n",
      "model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.5.intermediate.dense.weight\n",
      "model.encoder.layer.5.intermediate.dense.bias\n",
      "model.encoder.layer.5.output.dense.weight\n",
      "model.encoder.layer.5.output.dense.bias\n",
      "model.encoder.layer.5.output.LayerNorm.weight\n",
      "model.encoder.layer.5.output.LayerNorm.bias\n",
      "model.encoder.layer.6.attention.self.query.weight\n",
      "model.encoder.layer.6.attention.self.query.bias\n",
      "model.encoder.layer.6.attention.self.key.weight\n",
      "model.encoder.layer.6.attention.self.key.bias\n",
      "model.encoder.layer.6.attention.self.value.weight\n",
      "model.encoder.layer.6.attention.self.value.bias\n",
      "model.encoder.layer.6.attention.output.dense.weight\n",
      "model.encoder.layer.6.attention.output.dense.bias\n",
      "model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.6.intermediate.dense.weight\n",
      "model.encoder.layer.6.intermediate.dense.bias\n",
      "model.encoder.layer.6.output.dense.weight\n",
      "model.encoder.layer.6.output.dense.bias\n",
      "model.encoder.layer.6.output.LayerNorm.weight\n",
      "model.encoder.layer.6.output.LayerNorm.bias\n",
      "model.encoder.layer.7.attention.self.query.weight\n",
      "model.encoder.layer.7.attention.self.query.bias\n",
      "model.encoder.layer.7.attention.self.key.weight\n",
      "model.encoder.layer.7.attention.self.key.bias\n",
      "model.encoder.layer.7.attention.self.value.weight\n",
      "model.encoder.layer.7.attention.self.value.bias\n",
      "model.encoder.layer.7.attention.output.dense.weight\n",
      "model.encoder.layer.7.attention.output.dense.bias\n",
      "model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.7.intermediate.dense.weight\n",
      "model.encoder.layer.7.intermediate.dense.bias\n",
      "model.encoder.layer.7.output.dense.weight\n",
      "model.encoder.layer.7.output.dense.bias\n",
      "model.encoder.layer.7.output.LayerNorm.weight\n",
      "model.encoder.layer.7.output.LayerNorm.bias\n",
      "model.encoder.layer.8.attention.self.query.weight\n",
      "model.encoder.layer.8.attention.self.query.bias\n",
      "model.encoder.layer.8.attention.self.key.weight\n",
      "model.encoder.layer.8.attention.self.key.bias\n",
      "model.encoder.layer.8.attention.self.value.weight\n",
      "model.encoder.layer.8.attention.self.value.bias\n",
      "model.encoder.layer.8.attention.output.dense.weight\n",
      "model.encoder.layer.8.attention.output.dense.bias\n",
      "model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.8.intermediate.dense.weight\n",
      "model.encoder.layer.8.intermediate.dense.bias\n",
      "model.encoder.layer.8.output.dense.weight\n",
      "model.encoder.layer.8.output.dense.bias\n",
      "model.encoder.layer.8.output.LayerNorm.weight\n",
      "model.encoder.layer.8.output.LayerNorm.bias\n",
      "model.encoder.layer.9.attention.self.query.weight\n",
      "model.encoder.layer.9.attention.self.query.bias\n",
      "model.encoder.layer.9.attention.self.key.weight\n",
      "model.encoder.layer.9.attention.self.key.bias\n",
      "model.encoder.layer.9.attention.self.value.weight\n",
      "model.encoder.layer.9.attention.self.value.bias\n",
      "model.encoder.layer.9.attention.output.dense.weight\n",
      "model.encoder.layer.9.attention.output.dense.bias\n",
      "model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.9.intermediate.dense.weight\n",
      "model.encoder.layer.9.intermediate.dense.bias\n",
      "model.encoder.layer.9.output.dense.weight\n",
      "model.encoder.layer.9.output.dense.bias\n",
      "model.encoder.layer.9.output.LayerNorm.weight\n",
      "model.encoder.layer.9.output.LayerNorm.bias\n",
      "model.encoder.layer.10.attention.self.query.weight\n",
      "model.encoder.layer.10.attention.self.query.bias\n",
      "model.encoder.layer.10.attention.self.key.weight\n",
      "model.encoder.layer.10.attention.self.key.bias\n",
      "model.encoder.layer.10.attention.self.value.weight\n",
      "model.encoder.layer.10.attention.self.value.bias\n",
      "model.encoder.layer.10.attention.output.dense.weight\n",
      "model.encoder.layer.10.attention.output.dense.bias\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.10.intermediate.dense.weight\n",
      "model.encoder.layer.10.intermediate.dense.bias\n",
      "model.encoder.layer.10.output.dense.weight\n",
      "model.encoder.layer.10.output.dense.bias\n",
      "model.encoder.layer.10.output.LayerNorm.weight\n",
      "model.encoder.layer.10.output.LayerNorm.bias\n",
      "model.encoder.layer.11.attention.self.query.weight\n",
      "model.encoder.layer.11.attention.self.query.bias\n",
      "model.encoder.layer.11.attention.self.key.weight\n",
      "model.encoder.layer.11.attention.self.key.bias\n",
      "model.encoder.layer.11.attention.self.value.weight\n",
      "model.encoder.layer.11.attention.self.value.bias\n",
      "model.encoder.layer.11.attention.output.dense.weight\n",
      "model.encoder.layer.11.attention.output.dense.bias\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "model.encoder.layer.11.intermediate.dense.weight\n",
      "model.encoder.layer.11.intermediate.dense.bias\n",
      "model.encoder.layer.11.output.dense.weight\n",
      "model.encoder.layer.11.output.dense.bias\n",
      "model.encoder.layer.11.output.LayerNorm.weight\n",
      "model.encoder.layer.11.output.LayerNorm.bias\n",
      "model.pooler.dense.weight\n",
      "model.pooler.dense.bias\n",
      "matrix.weight\n",
      "matrix.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in my_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
